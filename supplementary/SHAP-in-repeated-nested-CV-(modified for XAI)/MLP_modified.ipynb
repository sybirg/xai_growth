{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4687bc38",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Imaging-AI-for-Health-virtual-lab/SHAP-in-repeated-nested-CV/blob/main/regression_ICBM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee6ab2",
   "metadata": {},
   "source": [
    "# REPEATED NESTED CROSS-VALIDATION FOR Multilayer perceptron - Testing on glucose DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c09bc30",
   "metadata": {},
   "source": [
    "Cloning GitHub repository:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ae8ad",
   "metadata": {},
   "source": [
    "Install dependencies and import modules (Unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Import packages ########################\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle, keras\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3b401",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_shap_values(dir_name, data, num_trials, num_splits):\n",
    "    \"\"\"\n",
    "    Takes shap values from .pkl files computed for different folds and trials and makes a mathematical average, and plots the final results.\n",
    "    --------------------------\n",
    "    Parameters:\n",
    "\n",
    "    - dir_name : <string>, path in which there is the directory \"Shap_values/\"\n",
    "    - data : <pandas dataframe>, data used to compute SHAP values\n",
    "    - num_trials : <int>, number of repetitions\n",
    "    - num_splits: <int>, number of splits for the nested cross-validation\n",
    "    ------------------------\n",
    "\n",
    "    Returns:\n",
    "    - shaps: <list>, list of two pandas Dataframes containing the SHAP values for training set and test set separately.\n",
    "\n",
    "    \"\"\"\n",
    "    shaps = []\n",
    "    for name in [\"train\", \"test\"]:\n",
    "        final_shap_values = None\n",
    "        for num_trial in range(num_trials):\n",
    "            for num_split in range(num_splits):\n",
    "                file = open(dir_name + \"Shap_values/\" + name + str(num_trial) + \"fold\" + str(num_split) + \".pkl\", \"rb\")\n",
    "                shap_values = pickle.load(file)\n",
    "                if final_shap_values is None:\n",
    "                    final_shap_values = shap_values\n",
    "                else:\n",
    "                    final_shap_values = final_shap_values.append(shap_values)\n",
    "\n",
    "        features = list(shap_values.columns)\n",
    "        features.remove(\"index\")\n",
    "\n",
    "        df = pd.DataFrame(columns=features)\n",
    "        for i in range(len(data)):\n",
    "            filt = final_shap_values['index'] == i\n",
    "\n",
    "            df = df.append(final_shap_values[filt].sum(axis=0, numeric_only=True), ignore_index=True)\n",
    "        del df[\"index\"]\n",
    "\n",
    "        if \"train\" in name:\n",
    "            df = df / num_trials / (num_splits - 1)\n",
    "        else:\n",
    "            df = df / num_trials\n",
    "\n",
    "        shaps.append(df)\n",
    "\n",
    "    return shaps\n",
    "\n",
    "def kernel_constraint_func(int):\n",
    "    if int == -1:\n",
    "        return None\n",
    "    elif int == 2:\n",
    "        return tf.keras.constraints.max_norm(2)\n",
    "    elif int == 3:\n",
    "        return tf.keras.constraints.max_norm(3)\n",
    "    elif int == 4:\n",
    "        return tf.keras.constraints.max_norm(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ff017",
   "metadata": {},
   "source": [
    "## Setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26844be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####### SETTING VARIABLES ########################\n",
    "experiment_name = \"XAI\"\n",
    "dir_name = \"nested_cv_keras/\"\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "os.makedirs(dir_name + \"Shap_values\", exist_ok=True)\n",
    "os.makedirs(dir_name + \"plots\", exist_ok=True)\n",
    "os.makedirs(dir_name + \"average_plots\", exist_ok=True)\n",
    "\n",
    "num_trials = 10 # Number of repetitions (Decreased to 10, about 16 hours computation time)\n",
    "num_splits = 5 #Number of folds (Default)\n",
    "n_iter= 1000 #Number of iteration for hyperparameter tuning (We increased to 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d55e3b",
   "metadata": {},
   "source": [
    "## Reading and filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca721ea",
   "metadata": {},
   "source": [
    "Input data (Glucose condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_raw  = pd.read_feather(\"data/simulated_fluxes(glc).feather\").set_index(\"index\")\n",
    "\n",
    "#Remove any unnecessary columns(reactions)\n",
    "X_data  = pd.DataFrame(index= X_data_raw.index)\n",
    "for index_col in X_data_raw.columns:\n",
    "    each_column = X_data_raw.loc[:,index_col]\n",
    "    is_neg = 0\n",
    "    is_pos = 0\n",
    "    for f_value in each_column:\n",
    "        if f_value <0 :\n",
    "            is_neg =1\n",
    "        if f_value > 0:\n",
    "            is_pos =1\n",
    "\n",
    "    if  is_neg ==0 and is_pos==0:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        X_data[each_column.name] = abs(each_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fd973",
   "metadata": {},
   "source": [
    "Output data (Glucose condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b37a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting growth data for target data\n",
    "growth_data = pd.read_feather(\"data/biomass_data.feather\").set_index(\"index\")\n",
    "y_data_raw =  growth_data[\"glc\"]\n",
    "y_data = y_data_raw[y_data_raw.index.isin(X_data.index)]\n",
    "\n",
    "features = list(X_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923518cf",
   "metadata": {},
   "source": [
    "## Defining variables and metrices for nested cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d459ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = np.zeros(num_trials)\n",
    "test_scores = np.zeros(num_trials)\n",
    "\n",
    "mse_train_scores = np.zeros(num_trials)\n",
    "r2_train_scores = np.zeros(num_trials)\n",
    "neg_mean_absolute_error_train_scores = np.zeros(num_trials)\n",
    "correlation_train_scores = np.zeros(num_trials)\n",
    "\n",
    "mse_test_scores = np.zeros(num_trials)\n",
    "r2_test_scores = np.zeros(num_trials)\n",
    "neg_mean_absolute_error_test_scores = np.zeros(num_trials)\n",
    "correlation_test_scores = np.zeros(num_trials)\n",
    "\n",
    "\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    return np.corrcoef(y_true,y_pred)[0][1]\n",
    "\n",
    "score = make_scorer(my_custom_loss_func)\n",
    "\n",
    "myscoring = {'mse': 'neg_mean_squared_error',\n",
    "        'r2': 'r2',\n",
    "        'neg_mean_absolute_error':'neg_mean_absolute_error',\n",
    "        'correlation' : score\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da046e",
   "metadata": {},
   "source": [
    "## Model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af65f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We reduced the size of hyperparameter space, but made sure to include parameter that were selected in the previous deep learning tuning.\n",
    "\n",
    "######## Hyperparamter SETTING ##############################\n",
    "layers = [0,1,2,3]\n",
    "neurons           = [10, 25, 50, 100, 200, 1000] # number of perceptrons for each layers\n",
    "optimizer_param   = ['adam', 'rmsprop', 'sgd'] # backpropagation optimizers\n",
    "learning_rate     = [0.1, 0.01,0.005,0.001]\n",
    "kernel_constraint = [-1,2,3] # layer weight constraints, -1 : no constraint\n",
    "dropout           = [0.4, 0.5, 0.6] # Dropout layer rate\n",
    "\n",
    "\n",
    "######## MODEL SETTING ##############################\n",
    "def build_model(layers, neurons,optimizer_param,learning_rate,kernel_constraint,dropout ):\n",
    "    # Model construction\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(len(X_data.columns),)))\n",
    "    for i in range(0,layers+1):\n",
    "        model.add(tf.keras.layers.Dense(units= neurons, activation='relu',\n",
    "                                        kernel_constraint=kernel_constraint_func(\n",
    "                                            kernel_constraint)))\n",
    "        model.add(tf.keras.layers.Dropout(dropout))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optimizer_param\n",
    "\n",
    "    if optimizer == \"adam\":\n",
    "        final_optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    elif optimizer == \"sgd\":\n",
    "        final_optimizer = tf.optimizers.SGD(learning_rate)\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        final_optimizer = tf.optimizers.RMSprop(learning_rate)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=final_optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "temp_model = KerasRegressor(build_fn=build_model)\n",
    "\n",
    "params =  dict(layers = layers,\n",
    "               neurons =neurons,\n",
    "               optimizer_param = optimizer_param,\n",
    "               learning_rate=learning_rate,\n",
    "               kernel_constraint=kernel_constraint,\n",
    "               dropout=dropout\n",
    "               )\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094f54f",
   "metadata": {},
   "source": [
    "## Start of repeated nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a024f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sklearn.preprocessing.StandardScaler().fit_transform(X_data)\n",
    "X = pd.DataFrame(X, columns=features)\n",
    "y = y_data\n",
    "\n",
    "\n",
    "shap.initjs()\n",
    "for i in range(num_trials):\n",
    "\n",
    "    print(\"Iteration:\" + str(i))\n",
    "\n",
    "    inner_cv = KFold(n_splits=num_splits, shuffle=True, random_state=i)\n",
    "    outer_cv = KFold(n_splits=num_splits, shuffle=True, random_state=i)\n",
    "\n",
    "    \n",
    "    clf = RandomizedSearchCV(estimator=temp_model, param_distributions=params, n_iter=n_iter, scoring=score,\n",
    "                            refit=True, cv=inner_cv, verbose=0,  random_state=i, return_train_score=True,\n",
    "                            n_jobs =1) \n",
    "\n",
    "\n",
    "    nested_score = cross_validate(clf, X=X, y=y, cv=outer_cv, return_train_score=True,return_estimator=True, scoring = myscoring)\n",
    "\n",
    "\n",
    "    mse_test_scores[i] = np.mean(nested_score['test_mse'])\n",
    "    r2_test_scores[i] = np.mean(nested_score['test_r2'])\n",
    "    neg_mean_absolute_error_test_scores[i] = np.mean(nested_score['test_neg_mean_absolute_error'])\n",
    "    correlation_test_scores[i] = np.mean(nested_score['test_correlation'])\n",
    "\n",
    "    print('mse ' + str(mse_test_scores[i]))\n",
    "    print('r2 ' + str(r2_test_scores[i]))\n",
    "    print('neg_mean_absolute_error ' + str(neg_mean_absolute_error_test_scores[i]))\n",
    "    print('correlation ' + str(correlation_test_scores[i]))\n",
    "\n",
    "\n",
    "    ############# SHAP VALUES COMPUTATION FOR EACH FOLD ##################\n",
    "    iter_shap = 0\n",
    "    indices = []\n",
    "\n",
    "    for train_index, test_index in inner_cv.split(X, y):\n",
    "        #print(\"Split:\", iter_shap)\n",
    "        ## TRUE POSITIVE RATE COMPUTATION FOR EACH OUTER LOOP (TEST SET)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train = pd.DataFrame(X_train,columns=features)\n",
    "        X_test = pd.DataFrame(X_test,columns=features)\n",
    "        y_train = pd.DataFrame(y_train)\n",
    "\n",
    "\n",
    "        regressor_model = nested_score['estimator'][iter_shap].best_estimator_.model\n",
    "        print(regressor_model.summary())\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        X_test = np.array(X_test)\n",
    "\n",
    "        regressor_model.fit(X_train, y_train, epochs =40, verbose=0)\n",
    "        #y_pred = regressor_model.predict(X_test)\n",
    "\n",
    "        #train_tmp_df = pd.DataFrame(X_train, columns=features)\n",
    "        explainer = shap.DeepExplainer(regressor_model, X_train)\n",
    "        df_shap = pd.DataFrame(explainer.shap_values(X_train,check_additivity=False)[0], columns=features)\n",
    "        df_shap[\"index\"] = train_index\n",
    "        pickle.dump(df_shap,\n",
    "                    open(dir_name + '/Shap_values/' + 'train' + str(i) + 'fold' + str(iter_shap) + '.pkl', 'wb'))\n",
    "\n",
    "\n",
    "        #test shap\n",
    "        test_explainer = shap.DeepExplainer(regressor_model,X_test)\n",
    "        test_df_shap = pd.DataFrame(explainer.shap_values(X_test,check_additivity=False)[0], columns=features)\n",
    "        test_df_shap[\"index\"] = test_index\n",
    "        pickle.dump(test_df_shap,\n",
    "                    open(dir_name + '/Shap_values/' + 'test' + str(i) + 'fold' + str(iter_shap) + '.pkl', 'wb'))\n",
    "\n",
    "\n",
    "        iter_shap += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a74597",
   "metadata": {
    "id": "b828cca2"
   },
   "source": [
    "# AVERAGE SHAP VALUES FOR TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829dfdbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MSgpEFq4SQeF",
    "outputId": "78eb2f18-aa4e-4920-bae0-28222e3661fc"
   },
   "outputs": [],
   "source": [
    "averaged_shaps = average_shap_values(dir_name,X_data_raw[features], num_trials,num_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd8473",
   "metadata": {},
   "source": [
    "## TRAIN SHAP VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e30190",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_shap=averaged_shaps[0]\n",
    "training_shap_df_median = pd.DataFrame(training_shap.median())\n",
    "training_shap_df_median.columns = [\"SHAP value\"]\n",
    "training_shap_df_median = training_shap_df_median.sort_values([\"SHAP value\"])\n",
    "\n",
    "#Extract each reaction's SHAP value\n",
    "raw_SHAP_values = training_shap_df_median\n",
    "\n",
    "#Filter out transport and external reactions\n",
    "memote_pure_rxn = open(\"util/memote_pure_rxns.txt\", 'r').read().strip('\"').split('\",\"')\n",
    "\n",
    "#Separate beneficial(+) and detrimental(-) reactions based on SHAP value\n",
    "SHAP_pos = raw_SHAP_values[raw_SHAP_values.iloc[:, 0] > 0]\n",
    "SHAP_neg = raw_SHAP_values[raw_SHAP_values.iloc[:, 0] < 0]\n",
    "\n",
    "#Filter out reactions with negligible SHAP value\n",
    "avg_coefs_pos = SHAP_pos.iloc[:, 0].mean()\n",
    "avg_coefs_neg = SHAP_neg.iloc[:, 0].mean()\n",
    "\n",
    "final_pos_SHAPs = SHAP_pos[SHAP_pos.iloc[:,0] >=  0.1*avg_coefs_pos]\n",
    "final_pos_SHAPs = final_pos_SHAPs[final_pos_SHAPs.index.isin(memote_pure_rxn) == True]\n",
    "final_neg_SHAPs = SHAP_neg[abs(SHAP_neg.iloc[:,0]) >= abs(0.1*avg_coefs_neg)]\n",
    "final_neg_SHAPs = final_neg_SHAPs[final_neg_SHAPs.index.isin(memote_pure_rxn) == True]\n",
    "\n",
    "#Sort and extract to csv\n",
    "filtered_SHAPs = final_pos_SHAPs.append(final_neg_SHAPs)\n",
    "filtered_SHAPs = filtered_SHAPs.sort_values(ascending=False, by=[\"SHAP value\"])\n",
    "filtered_SHAPs.to_csv(\"output/training_shap.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215219f4",
   "metadata": {},
   "source": [
    "## TEST SHAP VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_shap = averaged_shaps[1]\n",
    "testing_shap_median = pd.DataFrame(testing_shap.median())\n",
    "testing_shap_median.columns = [\"SHAP value\"]\n",
    "testing_shap_median = testing_shap_median.sort_values([\"SHAP value\"])\n",
    "\n",
    "#Extract each reaction's SHAP value\n",
    "raw_SHAP_values = testing_shap_median\n",
    "\n",
    "#Filter out transport and external reactions\n",
    "memote_pure_rxn = open(\"util/memote_pure_rxns.txt\", 'r').read().strip('\"').split('\",\"')\n",
    "\n",
    "#Separate beneficial(+) and detrimental(-) reactions based on SHAP value\n",
    "SHAP_pos = raw_SHAP_values[raw_SHAP_values.iloc[:, 0] > 0]\n",
    "SHAP_neg = raw_SHAP_values[raw_SHAP_values.iloc[:, 0] < 0]\n",
    "\n",
    "#Filter out reactions with negligible SHAP value\n",
    "avg_coefs_pos = SHAP_pos.iloc[:, 0].mean()\n",
    "avg_coefs_neg = SHAP_neg.iloc[:, 0].mean()\n",
    "\n",
    "final_pos_SHAPs = SHAP_pos[SHAP_pos.iloc[:,0] >=  0.1*avg_coefs_pos]\n",
    "final_pos_SHAPs = final_pos_SHAPs[final_pos_SHAPs.index.isin(memote_pure_rxn) == True]\n",
    "final_neg_SHAPs = SHAP_neg[abs(SHAP_neg.iloc[:,0]) >= abs(0.1*avg_coefs_neg)]\n",
    "final_neg_SHAPs = final_neg_SHAPs[final_neg_SHAPs.index.isin(memote_pure_rxn) == True]\n",
    "\n",
    "#Sort and extract to csv\n",
    "filtered_SHAPs = final_pos_SHAPs.append(final_neg_SHAPs)\n",
    "filtered_SHAPs = filtered_SHAPs.sort_values(ascending=False, by=[\"SHAP value\"])\n",
    "filtered_SHAPs.columns = [\"SHAP value\"] \n",
    "filtered_SHAPs.to_csv(\"output/test_shap.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a4b5b2d3bc25746aef17473cfa7043d0873c673e7ee22e0b1333aee054b345"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('dementia')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
