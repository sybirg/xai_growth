{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4687bc38",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Imaging-AI-for-Health-virtual-lab/SHAP-in-repeated-nested-CV/blob/main/regression_ICBM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee6ab2",
   "metadata": {},
   "source": [
    "# Tree-based Feature selection - Testing on glucose DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ae8ad",
   "metadata": {},
   "source": [
    "Install dependencies and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Import packages ########################\n",
    "import shap \n",
    "import sklearn \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from h2o4gpu.solvers.elastic_net import ElasticNet \n",
    "import h2o4gpu.util.import_data as io\n",
    "import h2o4gpu.util.metrics as metrics\n",
    "\n",
    "carbon_source = \"glc\" # glucose condition\n",
    "output_name = \"glc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3b401",
   "metadata": {},
   "source": [
    "Define training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_data_raw  = pd.read_feather(\"simulated_fluxes(\"+carbon_source+\").feather\").set_index(\"index\")\n",
    "X_train_scaled = sklearn.preprocessing.StandardScaler().fit_transform(X_data_raw)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_data_raw.columns)\n",
    "\n",
    "#Extracting growth data for target data\n",
    "growth_data = pd.read_feather(\"biomass_data.feather\").set_index(\"index\")\n",
    "y_data_raw =  growth_data[carbon_source]\n",
    "y_data = y_data_raw[y_data_raw.index.isin(X_data_raw.index)]\n",
    "y_train =y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ff017",
   "metadata": {},
   "source": [
    "feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26844be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=0)\n",
    "rf.fit(X_train_scaled,y_train)\n",
    "rf_model = SelectFromModel(rf, prefit=True)\n",
    "X_train_new = rf_model.transform(X_train_scaled)\n",
    "rf_model.get_support()\n",
    "selector_true =[ i for i, f in enumerate(rf_model.get_support()) if f ]\n",
    "selector_rxn = [X_data_raw.columns[i] for i in selector_true]\n",
    "X_data = pd.DataFrame(X_train_new, columns=selector_rxn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca721ea",
   "metadata": {},
   "source": [
    "Model training and reaction prediction with ElasticNet regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0 # random seed\n",
    "n_alphas    = 100 # number of alphas along the regularization path\n",
    "max_iter    = 1e4 # maximum number of iterations\n",
    "tol         = 1e-6 # tolerance for the optimization\n",
    "cv_folds    = 300 # number of cross validation folds\n",
    "l1_ratio    = 1e-2 # scaling between l1 and l2 penalties\n",
    "\n",
    "# Standardize data\n",
    "\n",
    "# Shuffle the data\n",
    "y_train = y_data\n",
    "\n",
    "X_train_train, y_train = sklearn.utils.shuffle(X_data, y_train, random_state=random_seed)\n",
    "\n",
    "# Train the data\n",
    "enlr = ElasticNet(max_iter=max_iter,\n",
    "                  n_alphas=n_alphas,\n",
    "                  tol=tol,\n",
    "                  n_folds=cv_folds,\n",
    "                  l1_ratio=l1_ratio,\n",
    "                  random_state=random_seed\n",
    "                  )\n",
    "\n",
    "enlr.fit(X_train_train, y_train)\n",
    "\n",
    "\n",
    "#Extract each reaction's coefficient\n",
    "raw_coefs_data = pd.Series(enlr.coef_, index=X_data.columns , name=  \"Coefficient\").to_frame()\n",
    "\n",
    "#Filter out transport and external reactions\n",
    "memote_pure_rxn = open(\"util/memote_pure_rxns.txt\", 'r').read().strip('\"').split('\",\"')\n",
    "\n",
    "#Separate beneficial(+) and detrimental(-) reactions based on coefficient value\n",
    "coefs_pos = raw_coefs_data[raw_coefs_data.iloc[:, 0] > 0]\n",
    "coefs_neg = raw_coefs_data[raw_coefs_data.iloc[:, 0] < 0]\n",
    "\n",
    "#Filter out reactions with negligible coefficient value\n",
    "avg_coefs_pos = coefs_pos.iloc[:, 0].mean()\n",
    "avg_coefs_neg = coefs_neg.iloc[:, 0].mean()\n",
    "\n",
    "final_pos_coefs = coefs_pos[coefs_pos.iloc[:,0] >=  0.1*avg_coefs_pos]\n",
    "final_pos_coefs = final_pos_coefs[final_pos_coefs.index.isin(memote_pure_rxn) == True]\n",
    "final_neg_coefs = coefs_neg[abs(coefs_neg.iloc[:,0]) >= abs(0.1*avg_coefs_neg)]\n",
    "final_neg_coefs = final_neg_coefs[final_neg_coefs.index.isin(memote_pure_rxn) == True]\n",
    "\n",
    "#Sort and extract to csv\n",
    "filtered_coefs = final_pos_coefs.append(final_neg_coefs)\n",
    "filtered_coefs  = filtered_coefs.sort_values(ascending=True, by=\"Coefficient\")\n",
    "filtered_coefs.to_csv(\"output/glc_en_tree.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fd973",
   "metadata": {},
   "source": [
    "Model training and reaction prediction with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14492f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(len(X_data.columns),)),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "        ])\n",
    "model.compile(optimizer=tf.optimizers.RMSprop(lr=0.005), loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "# Set the list of random seeds for MLP training & SHAP values\n",
    "seed_num_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "total_shap_df = pd.DataFrame(index=X_data.columns)\n",
    "\n",
    "for seed_num in seed_num_list:\n",
    "    tf.random.set_seed(seed_num)\n",
    "    random.seed(seed_num)\n",
    "\n",
    "    # Standardize data\n",
    "    X_train_scaled = sklearn.preprocessing.StandardScaler().fit_transform(X_data)\n",
    "\n",
    "    y_train = y_data\n",
    "    # Shuffle data\n",
    "    X_train_scaled, y_train = sklearn.utils.shuffle(X_train_scaled, y_train, random_state=seed_num)\n",
    "\n",
    "    # Artificial Neural Network build\n",
    "    with tf.device(\"cpu:0\"):\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(len(X_data.columns),)),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "        ])\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer=tf.optimizers.RMSprop(lr=0.005), loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "        # Train model\n",
    "        model.fit(x=X_train_scaled, y=y_train, epochs=40, validation_split=0.1)\n",
    "\n",
    "    # SHAP computation\n",
    "    background = X_train_scaled\n",
    "    explainer = shap.DeepExplainer(model, background)  # create the background set\n",
    "    shap_values = explainer.shap_values(X_train_scaled)  # train the explainer\n",
    "    shap_df = pd.DataFrame(shap_values[0], columns=X_data.columns)\n",
    "    median_shap = pd.DataFrame(shap_df.median())\n",
    "    # median_shap = median_shap.sort_values(ascending=False)\n",
    "    total_shap_df = pd.merge(total_shap_df, median_shap, left_index=True, right_index=True)\n",
    "\n",
    "# The average SHAP values will be the representative for each features\n",
    "total_shap_df_mean = total_shap_df.mean(axis=1)\n",
    "\n",
    "total_shap_df_mean = total_shap_df_mean.sort_values(ascending=False)\n",
    "\n",
    "#Extract each reaction's SHAP value\n",
    "raw_SHAP_values = total_shap_df_mean.to_frame()\n",
    "\n",
    "#Filter out transport and external reactions\n",
    "memote_pure_rxn = open(\"util/memote_pure_rxns.txt\", 'r').read().strip('\"').split('\",\"')\n",
    "\n",
    "#Separate beneficial(+) and detrimental(-) reactions based on SHAP value\n",
    "SHAP_pos = raw_SHAP_values[raw_SHAP_values.iloc[:, 0] > 0]\n",
    "SHAP_neg = raw_SHAP_values[raw_SHAP_values.iloc[:, 0] < 0]\n",
    "\n",
    "#Filter out reactions with negligible SHAP value\n",
    "avg_coefs_pos = SHAP_pos.iloc[:, 0].mean()\n",
    "avg_coefs_neg = SHAP_neg.iloc[:, 0].mean()\n",
    "\n",
    "final_pos_SHAPs = SHAP_pos[SHAP_pos.iloc[:,0] >=  0.1*avg_coefs_pos]\n",
    "final_pos_SHAPs = final_pos_SHAPs[final_pos_SHAPs.index.isin(memote_pure_rxn) == True]\n",
    "final_neg_SHAPs = SHAP_neg[abs(SHAP_neg.iloc[:,0]) >= abs(0.1*avg_coefs_neg)]\n",
    "final_neg_SHAPs = final_neg_SHAPs[final_neg_SHAPs.index.isin(memote_pure_rxn) == True]\n",
    "\n",
    "#Sort and extract to csv\n",
    "filtered_SHAPs = final_pos_SHAPs.append(final_neg_SHAPs)\n",
    "filtered_SHAPs = filtered_SHAPs.sort_values(ascending=False, by=0)\n",
    "filtered_SHAPs.columns = [\"SHAP value\"]\n",
    "filtered_SHAPs.to_csv(\"output/glc_mlp_tree.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a4b5b2d3bc25746aef17473cfa7043d0873c673e7ee22e0b1333aee054b345"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('dementia')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
