{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4687bc38",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Imaging-AI-for-Health-virtual-lab/SHAP-in-repeated-nested-CV/blob/main/regression_ICBM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee6ab2",
   "metadata": {},
   "source": [
    "# Tree-based Feature selection - Testing on a glucose DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ae8ad",
   "metadata": {},
   "source": [
    "Install dependencies and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Import packages ########################\n",
    "import shap \n",
    "import sklearn \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import kerastuner as kt\n",
    "from h2o4gpu.solvers.elastic_net import ElasticNet \n",
    "import h2o4gpu.util.import_data as io\n",
    "import h2o4gpu.util.metrics as metrics\n",
    "\n",
    "carbon_source = \"glc\" # glucose condition\n",
    "output_name = \"glc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3b401",
   "metadata": {},
   "source": [
    "### 0. Define training dataset\n",
    "* Prior to feature selection, the data was first normalized using StandardScaler.\n",
    "* Then variance threshold was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting metabolic flux\n",
    "X_data_raw  = pd.read_feather(\"simulated_fluxes(\"+carbon_source+\").feather\").set_index(\"index\")\n",
    "X_train_scaled = sklearn.preprocessing.StandardScaler().fit_transform(X_data_raw)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_data_raw.columns)\n",
    "\n",
    "#Extracting growth data for target data\n",
    "growth_data = pd.read_feather(\"biomass_data.feather\").set_index(\"index\")\n",
    "y_data_raw =  growth_data[carbon_source]\n",
    "y_data = y_data_raw[y_data_raw.index.isin(X_data_raw.index)]\n",
    "y_train = y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ff017",
   "metadata": {},
   "source": [
    "Feature selection (Random forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26844be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=0)\n",
    "rf.fit(X_train_scaled,y_train)\n",
    "rf_model = SelectFromModel(rf, prefit=True)\n",
    "X_train_new = rf_model.transform(X_train_scaled)\n",
    "rf_model.get_support()\n",
    "selector_true =[ i for i, f in enumerate(rf_model.get_support()) if f ]\n",
    "selector_rxn = [X_data_raw.columns[i] for i in selector_true]\n",
    "X_data = pd.DataFrame(X_train_new, columns=selector_rxn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca721ea",
   "metadata": {},
   "source": [
    "### 1. Model training and prediction with ElasticNet regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0 # random seed\n",
    "n_alphas    = 100 # number of alphas along the regularization path\n",
    "max_iter    = 1e4 # maximum number of iterations\n",
    "tol         = 1e-6 # tolerance for the optimization\n",
    "cv_folds    = 300 # number of cross validation folds\n",
    "l1_ratio    = 1e-2 # scaling between l1 and l2 penalties\n",
    "\n",
    "# Shuffle the data\n",
    "X_train = X_data\n",
    "y_train = y_data\n",
    "X_train, y_train = sklearn.utils.shuffle(X_train, y_train, random_state=random_seed)\n",
    "\n",
    "# Train the data\n",
    "enlr = ElasticNet(max_iter=max_iter,\n",
    "                  n_alphas=n_alphas,\n",
    "                  tol=tol,\n",
    "                  n_folds=cv_folds,\n",
    "                  l1_ratio=l1_ratio,\n",
    "                  random_state=random_seed\n",
    "                  )\n",
    "\n",
    "enlr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Extract each reaction's coefficient\n",
    "raw_coefs_data = pd.Series(enlr.coef_, index=X_data.columns , name=  \"Coefficient\").to_frame()\n",
    "\n",
    "#Filter out transport and external reactions\n",
    "memote_pure_rxn = open(\"util/memote_pure_rxns.txt\", 'r').read().strip('\"').split('\",\"')\n",
    "\n",
    "#Separate beneficial(+) and detrimental(-) reactions based on coefficient value\n",
    "coefs_pos = raw_coefs_data[raw_coefs_data.iloc[:, 0] > 0]\n",
    "coefs_neg = raw_coefs_data[raw_coefs_data.iloc[:, 0] < 0]\n",
    "\n",
    "#Filter out reactions with negligible coefficient value\n",
    "avg_coefs_pos = coefs_pos.iloc[:, 0].mean()\n",
    "avg_coefs_neg = coefs_neg.iloc[:, 0].mean()\n",
    "\n",
    "final_pos_coefs = coefs_pos[coefs_pos.iloc[:,0] >=  0.1*avg_coefs_pos]\n",
    "final_pos_coefs = final_pos_coefs[final_pos_coefs.index.isin(memote_pure_rxn) == True]\n",
    "final_neg_coefs = coefs_neg[abs(coefs_neg.iloc[:,0]) >= abs(0.1*avg_coefs_neg)]\n",
    "final_neg_coefs = final_neg_coefs[final_neg_coefs.index.isin(memote_pure_rxn) == True]\n",
    "\n",
    "#Sort and extract to csv\n",
    "filtered_coefs = final_pos_coefs.append(final_neg_coefs)\n",
    "filtered_coefs  = filtered_coefs.sort_values(ascending=True, by=\"Coefficient\")\n",
    "filtered_coefs.to_csv(\"output/glc_en_tree.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fd973",
   "metadata": {},
   "source": [
    "### 2. Model training and prediction with MLP\n",
    "\n",
    "First find the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed       = 0 # fix the seed for reproducability \n",
    "hp_dir            = \"hp_folder\" #hyperparameter tuning directory\n",
    "neurons           = [10,  50, 100, 200, 1000] # number of perceptrons for each layers \n",
    "optimizer_param   = [ 'rmsprop', 'sgd'] # backpropagation optimizers \n",
    "learning_rate     = [0.1,0.01,0.005,0.001] \n",
    "kernel_constraint = [-1,2,3,4] # layer weight constraints, -1 : no constraint\n",
    "dropout           = [0.4, 0.5, 0.6] # Dropout layer rate\n",
    "max_trials        = 1000\n",
    "\n",
    "\n",
    "#Shuffle the data\n",
    "X_train = X_data\n",
    "y_train = y_data\n",
    "\n",
    "X_train, y_train = sklearn.utils.shuffle(X_train, y_train, random_state=random_seed)\n",
    "\n",
    "#Layer weight regularizers\n",
    "def kernel_constraint_func(int):\n",
    "    if int ==-1:\n",
    "        return None\n",
    "    elif int ==2:\n",
    "        return tf.keras.constraints.max_norm(2)\n",
    "    elif int ==3:\n",
    "        return tf.keras.constraints.max_norm(3)\n",
    "    elif int ==4:\n",
    "        return tf.keras.constraints.max_norm(4)\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    #Model construction\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(len(X_data.columns),)))\n",
    "    for i in range(hp.Int('layers', 0,4)):\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Choice('units', neurons), activation='relu', kernel_constraint=kernel_constraint_func(hp.Choice(\"kernel\",kernel_constraint))))\n",
    "        model.add(tf.keras.layers.Dropout( hp.Choice('d_units',dropout)))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    #Optimizer \n",
    "    optimizer = hp.Choice('optimizer', values=optimizer_param)\n",
    "\n",
    "    if optimizer ==\"adam\":\n",
    "        final_optimizer  = tf.optimizers.Adam(hp.Choice('learning_rate', values=learning_rate))\n",
    "    elif optimizer == \"sgd\":\n",
    "        final_optimizer = tf.optimizers.SGD(hp.Choice('learning_rate', values=learning_rate))\n",
    "    elif optimizer ==\"rmsprop\":\n",
    "        final_optimizer = tf.optimizers.RMSprop(hp.Choice('learning_rate', values=learning_rate))\n",
    "\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer= final_optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mse']\n",
    "        )\n",
    "    return model\n",
    "\n",
    "#Tuning\n",
    "tuner = kt.RandomSearch(build_model, objective = 'val_mse',\n",
    "                        overwrite=True,\n",
    "                        max_trials=max_trials,\n",
    "                        executions_per_trial=3,\n",
    "                        directory=hp_dir,\n",
    "                        seed=random_seed)\n",
    "\n",
    "tuner.search(X_train, y_train, epochs = 40,validation_split =0.1, verbose=0)\n",
    "#tuner.search_space_summary()\n",
    "\n",
    "#Get the optimal hyperparameters\n",
    "best_hp=tuner.get_best_hyperparameters()\n",
    "print(\"Selected hp:\", best_hp[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected hp {'layers': 3, 'optimizer': 'rmsprop', 'learning_rate': 0.01, 'units': 50, 'kernel': 3, 'd_units': 0.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14492f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(len(X_data.columns),)),\n",
    "            tf.keras.layers.Dense(units=50, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.4),\n",
    "            tf.keras.layers.Dense(units=50, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.4),\n",
    "            tf.keras.layers.Dense(units=50, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.4),\n",
    "            tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "        ])\n",
    "model.compile(optimizer=tf.optimizers.RMSprop(lr=0.01), loss=\"mse\", metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and extract SHAP values from MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the list of random seeds for MLP training & SHAP values\n",
    "seed_num_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "total_shap_df = pd.DataFrame(index=X_data.columns)\n",
    "\n",
    "for seed_num in seed_num_list:\n",
    "    tf.random.set_seed(seed_num)\n",
    "    random.seed(seed_num)\n",
    "\n",
    "    # Standardize data\n",
    "    X_train = X_data\n",
    "    y_train = y_data\n",
    "    X_train, y_train = sklearn.utils.shuffle(X_train, y_train, random_state=seed_num)\n",
    "\n",
    "    # Artificial Neural Network build\n",
    "    with tf.device(\"cpu:0\"):\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(len(X_data.columns),)),\n",
    "            tf.keras.layers.Dense(units=50, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.4),\n",
    "            tf.keras.layers.Dense(units=50, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.4),\n",
    "            tf.keras.layers.Dense(units=50, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.4),\n",
    "            tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "        ])\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer=tf.optimizers.RMSprop(lr=0.01), loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "        # Train model\n",
    "        model.fit(x=X_train, y=y_train, epochs=40, validation_split=0.1)\n",
    "\n",
    "    # SHAP computation\n",
    "    background = X_train_new\n",
    "    explainer = shap.DeepExplainer(model, background)  # create the background set\n",
    "    shap_values = explainer.shap_values(X_train_new)  # train the explainer\n",
    "    shap_df = pd.DataFrame(shap_values[0], columns=X_data.columns)\n",
    "    median_shap = pd.DataFrame(shap_df.median())\n",
    "    # median_shap = median_shap.sort_values(ascending=False)\n",
    "    total_shap_df = pd.merge(total_shap_df, median_shap, left_index=True, right_index=True)\n",
    "\n",
    "# The average SHAP values will be the representative for each features\n",
    "total_shap_df_mean = total_shap_df.mean(axis=1)\n",
    "\n",
    "total_shap_df_mean = total_shap_df_mean.sort_values(ascending=False)\n",
    "\n",
    "#Extract each reaction's SHAP value\n",
    "raw_SHAP_values = total_shap_df_mean.to_frame()\n",
    "\n",
    "#Filter out transport and external reactions\n",
    "memote_pure_rxn = open(\"util/memote_pure_rxns.txt\", 'r').read().strip('\"').split('\",\"')\n",
    "\n",
    "#Separate beneficial(+) and detrimental(-) reactions based on SHAP value\n",
    "SHAP_pos = raw_SHAP_values[raw_SHAP_values.iloc[:, 0] > 0]\n",
    "SHAP_neg = raw_SHAP_values[raw_SHAP_values.iloc[:, 0] < 0]\n",
    "\n",
    "#Filter out reactions with negligible SHAP value\n",
    "avg_coefs_pos = SHAP_pos.iloc[:, 0].mean()\n",
    "avg_coefs_neg = SHAP_neg.iloc[:, 0].mean()\n",
    "\n",
    "final_pos_SHAPs = SHAP_pos[SHAP_pos.iloc[:,0] >=  0.1*avg_coefs_pos]\n",
    "final_pos_SHAPs = final_pos_SHAPs[final_pos_SHAPs.index.isin(memote_pure_rxn) == True]\n",
    "final_neg_SHAPs = SHAP_neg[abs(SHAP_neg.iloc[:,0]) >= abs(0.1*avg_coefs_neg)]\n",
    "final_neg_SHAPs = final_neg_SHAPs[final_neg_SHAPs.index.isin(memote_pure_rxn) == True]\n",
    "\n",
    "#Sort and extract to csv\n",
    "filtered_SHAPs = final_pos_SHAPs.append(final_neg_SHAPs)\n",
    "filtered_SHAPs = filtered_SHAPs.sort_values(ascending=False, by=0)\n",
    "filtered_SHAPs.columns = [\"SHAP value\"]\n",
    "filtered_SHAPs.to_csv(\"output/glc_mlp_tree.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14a4b5b2d3bc25746aef17473cfa7043d0873c673e7ee22e0b1333aee054b345"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('dementia')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
