{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron code\n",
    "\n",
    "* We begin with importing the necessary training data(metabolic fluxes & biomass).\n",
    "* Import the training data by typing the designated file name.\n",
    "\n",
    "| **Carbon** |    **acetate**   |  **adenosine**  | **D-alanine** |  **fructose**  |   **fucose**  |  **fumarate** | **galactose** | **galacturonate** | **gluconate** |      **glucosamine**     |\n",
    "|:----------:|:----------------:|:---------------:|:-------------:|:--------------:|:-------------:|:-------------:|:-------------:|:-----------------:|:-------------:|:------------------------:|\n",
    "|  file name |        ac        |       adn       |      alaD     |       fru      |      fuc      |      fum      |      gal      |       galur       |      glcn     |            gam           |\n",
    "| **Carbon** |    **glucose**   | **glucuronate** |  **glycerol** |   **lactate**  | **L-alanine** |   **malate**  |  **maltose**  |    **mannitol**   |  **mannose**  | **N-acetyl glucosamine** |\n",
    "|  file name |        glc       |      glcur      |      glyc     |       lac      |      alaL     |      mal      |      malt     |        mnl        |      man      |           acgam          |\n",
    "| **Carbon** | **oxaloacetate** |   **pyruvate**  |  **ribose**  | **saccharate** |  **sorbitol** | **succinate** | **thymidine** |   **trehalose**   |   **xylose**  |    **a-ketoglutarate**   |\n",
    "|  file name |        oaa       |       pyr       |      rib      |      sacc      |      sbt      |      succ     |     thymd     |        tre        |      xyl      |            akg           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_source = \"glc\" # glucose condition\n",
    "output_name = \"glc\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import package**\n",
    "\n",
    "For reproducibility, python & python packages' versions must be fixed as below.\n",
    "* python $\\;\\;\\;\\;\\;\\;$ :  v. 3.7\n",
    "* tensorflow $\\;$ :  v. 2.7.0\n",
    "* SHAP    $\\;\\;\\;\\;\\;\\;\\;$    :  v. 0.41.0\n",
    "* numpy  $\\;\\;\\;\\;$     : v. 1.21.6\n",
    "* pyarrow $\\;\\;\\;$ : v.  10.0.1\n",
    "* scikit-learn $\\;$ : v.  1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python v. 3.7.0\n",
    "import shap # v. 0.41.0\n",
    "import sklearn # v. 1.0.2\n",
    "import pandas as pd # v. 1.1.5\n",
    "import numpy as np # v. 1.21.6\n",
    "import tensorflow as tf # v. 2.7.0\n",
    "import keras_tuner as kt # v. 1.1.3\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import data**\n",
    "\n",
    "* The simulated flux data is imported and preprocessed for training data (X_train).\n",
    "* We absolutized each flux values and filtered out those that had constant value across all deletion mutants.\n",
    "* The final 24 OD data from Tong(2020) and Baba(2006) were used as target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting metabolic flux data\n",
    "X_data_raw  = pd.read_feather(\"input_data/simulated_fluxes(\"+carbon_source+\").feather\").set_index(\"index\")\n",
    "\n",
    "#Remove any unnecessary columns(reactions)\n",
    "X_data = pd.DataFrame(index=X_data_raw.index)\n",
    "for index_col in X_data_raw.columns:\n",
    "    each_column = X_data_raw.loc[:, index_col]\n",
    "    not_constant = 0\n",
    "    if_forst = 0\n",
    "    for f_value in each_column:\n",
    "\n",
    "        if if_forst == 0:\n",
    "            default_value = f_value\n",
    "            if_forst = 1\n",
    "        elif f_value != default_value:\n",
    "            not_constant = 1\n",
    "\n",
    "    if not_constant == 0 and f_value ==0:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        X_data[each_column.name] = abs(each_column)\n",
    "\n",
    "\n",
    "#Extracting growth data for target data\n",
    "growth_data = pd.read_feather(\"input_data/biomass_data.feather\").set_index(\"index\")\n",
    "y_data_raw =  growth_data[carbon_source]\n",
    "y_data = y_data_raw[y_data_raw.index.isin(X_data.index)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Deep learning with MLP**\n",
    "**Hyperparameter setting**\n",
    "* The optimal hyperparameters were determined with RandomSearch function in Keras-tuner.\n",
    "* Below are the lists of candidate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed       = 0 # fix the seed for reproducability \n",
    "hp_dir            = \"hp_folder\" #hyperparameter tuning directory\n",
    "neurons           = [5,10, 25, 50, 100, 200, 1000,2000] # number of perceptrons for each layers \n",
    "optimizer_param   = ['adam', 'rmsprop', 'sgd'] # backpropagation optimizers \n",
    "learning_rate     = [0.1,0.05,0.01,0.005,0.001,0.0001] \n",
    "kernel_constraint = [-1,2,3,4] # layer weight constraints, -1 : no constraint\n",
    "dropout           = [0.3,0.4, 0.5, 0.6] # Dropout layer rate\n",
    "max_trials        = 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize data\n",
    "X_train_scaled = sklearn.preprocessing.StandardScaler().fit_transform(X_data)\n",
    "y_train = y_data\n",
    "\n",
    "#Shuffle the data\n",
    "X_train_scaled, y_train = sklearn.utils.shuffle(X_train_scaled, y_train, random_state=random_seed)\n",
    "\n",
    "#Layer weight regularizers\n",
    "def kernel_constraint_func(int):\n",
    "    if int ==-1:\n",
    "        return None\n",
    "    elif int ==2:\n",
    "        return tf.keras.constraints.max_norm(2)\n",
    "    elif int ==3:\n",
    "        return tf.keras.constraints.max_norm(3)\n",
    "    elif int ==4:\n",
    "        return tf.keras.constraints.max_norm(4)\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    #Model construction\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(len(X_data.columns),)))\n",
    "    for i in range(hp.Int('layers', 0,4)):\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Choice('units', neurons), activation='relu', kernel_constraint=kernel_constraint_func(hp.Choice(\"kernel\",kernel_constraint))))\n",
    "        model.add(tf.keras.layers.Dropout( hp.Choice('d_units',dropout)))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    #Optimizer \n",
    "    optimizer = hp.Choice('optimizer', values=optimizer_param)\n",
    "\n",
    "    if optimizer ==\"adam\":\n",
    "        final_optimizer  = tf.optimizers.Adam(hp.Choice('learning_rate', values=learning_rate))\n",
    "    elif optimizer == \"sgd\":\n",
    "        final_optimizer = tf.optimizers.SGD(hp.Choice('learning_rate', values=learning_rate))\n",
    "    elif optimizer ==\"rmsprop\":\n",
    "        final_optimizer = tf.optimizers.RMSprop(hp.Choice('learning_rate', values=learning_rate))\n",
    "\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer= final_optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mse']\n",
    "        )\n",
    "    return model\n",
    "\n",
    "#Tuning\n",
    "tuner = kt.RandomSearch(build_model, objective = 'val_mse',\n",
    "                        overwrite=True,\n",
    "                        max_trials=max_trials,\n",
    "                        executions_per_trial=3,\n",
    "                        directory=hp_dir,\n",
    "                        seed=random_seed)\n",
    "\n",
    "tuner.search(X_train_scaled, y_train, epochs = 40,validation_split =0.1, verbose=0)\n",
    "#tuner.search_space_summary()\n",
    "\n",
    "\n",
    "#Get the optimal hyperparameters\n",
    "best_hp=tuner.get_best_hyperparameters()\n",
    "print(\"Selected hp:\", best_hp[0].values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training & SHAP value calculation**\n",
    "\n",
    "* The model architecture was constructed based on the selected parameters from hp tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected MLP model\n",
    "model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(len(X_data.columns),)),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "        ])\n",
    "model.compile(optimizer=tf.optimizers.RMSprop(lr=0.005), loss=\"mse\", metrics=[\"mse\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Due to the stochastic nature of deep learning MLP model, we used different random seeds for each model training.\n",
    " * For each model training, the 10% validation data was retained for monitering the model performance.\n",
    " * Each trained MLP model(with different random seeds) was consequently computed for SHAP values. \n",
    " * The average SHAP values across all models were assigned as the representative SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the list of random seeds for MLP training & SHAP values\n",
    "seed_num_list =[0,1,2,3,4,5,6,7,8,9]\n",
    "total_shap_df = pd.DataFrame(index=X_data.columns)\n",
    "\n",
    "for seed_num in seed_num_list:\n",
    "    tf.random.set_seed(seed_num)\n",
    "    random.seed(seed_num)\n",
    "\n",
    "    \n",
    "    #Standardize data\n",
    "    X_train_scaled = sklearn.preprocessing.StandardScaler().fit_transform(X_data)\n",
    "    y_train = y_data\n",
    "    #Shuffle data\n",
    "    X_train_scaled, y_train = sklearn.utils.shuffle(X_train_scaled, y_train, random_state=seed_num)\n",
    "    \n",
    "    #Artificial Neural Network build\n",
    "    with tf.device(\"cpu:0\"):\n",
    "\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(len(X_data.columns),)),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(units=1000, activation=\"relu\", kernel_constraint=tf.keras.constraints.max_norm(3)),\n",
    "            tf.keras.layers.Dropout(rate=0.6),\n",
    "            tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "        ])\n",
    "        \n",
    "        #Compile model\n",
    "        model.compile(optimizer=tf.optimizers.RMSprop(lr=0.005), loss=\"mse\", metrics=[\"mse\"])\n",
    "        \n",
    "        #Train model\n",
    "        model.fit(x=X_train_scaled, y=y_train, epochs=40, validation_split=0.1)\n",
    "\n",
    "\n",
    "    #SHAP computation\n",
    "    background = X_train_scaled\n",
    "    explainer = shap.DeepExplainer(model, background) # create the background set\n",
    "    shap_values = explainer.shap_values(X_train_scaled) # train the explainer \n",
    "    shap_df = pd.DataFrame(shap_values[0], columns=X_data.columns)\n",
    "    median_shap = pd.DataFrame(shap_df.median())\n",
    "    #median_shap = median_shap.sort_values(ascending=False)\n",
    "    total_shap_df = pd.merge(total_shap_df, median_shap, left_index=True,right_index=True)\n",
    "    \n",
    "# The average SHAP values will be the representative for each features\n",
    "total_shap_df_mean = total_shap_df.mean(axis=1) \n",
    "\n",
    "total_shap_df_mean = total_shap_df_mean.sort_values(ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract & filter the SHAP values of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract each reaction's SHAP value\n",
    "raw_SHAP_values = total_shap_df_mean.to_frame()\n",
    "\n",
    "#Filter out transport and external reactions\n",
    "memote_pure_rxn = open(\"util/memote_pure_rxns.txt\", 'r').read().strip('\"').split('\",\"')\n",
    "\n",
    "#Separate beneficial(+) and detrimental(-) reactions based on SHAP value\n",
    "SHAP_pos = raw_SHAP_values[raw_SHAP_values.iloc[:, 0] > 0]\n",
    "SHAP_neg = raw_SHAP_values[raw_SHAP_values.iloc[:, 0] < 0]\n",
    "\n",
    "#Filter out reactions with negligible SHAP value\n",
    "avg_coefs_pos = SHAP_pos.iloc[:, 0].mean()\n",
    "avg_coefs_neg = SHAP_neg.iloc[:, 0].mean()\n",
    "\n",
    "final_pos_SHAPs = SHAP_pos[SHAP_pos.iloc[:,0] >=  0.1*avg_coefs_pos]\n",
    "final_pos_SHAPs = final_pos_SHAPs[final_pos_SHAPs.index.isin(memote_pure_rxn) == True]\n",
    "final_neg_SHAPs = SHAP_neg[abs(SHAP_neg.iloc[:,0]) >= abs(0.1*avg_coefs_neg)]\n",
    "final_neg_SHAPs = final_neg_SHAPs[final_neg_SHAPs.index.isin(memote_pure_rxn) == True]\n",
    "\n",
    "#Sort and extract to csv\n",
    "filtered_SHAPs = final_pos_SHAPs.append(final_neg_SHAPs)\n",
    "filtered_SHAPs = filtered_SHAPs.sort_values(ascending=False, by=0)\n",
    "filtered_SHAPs.columns = [\"SHAP value\"] \n",
    "filtered_SHAPs.to_csv(\"MLP_output_data/\"+output_name+\"_mlp_shaps.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c257c458828b0801bd42c75066a4a4ab342d1b997385293a9445a5396be486a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
